model:
  arch: llava-1.5
  model_type: vicuna7b
  freeze_vit: True
  freeze_backbone: True
  tune_mm_mlp_adapter: False
  freeze_mm_mlp_adapter: True
  # Enable gradient checkpointing to save VRAM
  gradient_checkpointing: True
  max_txt_len: 64
  end_sym: "###"
  low_resource: False
  # prompt_path: "prompts/alignment.txt"
  prompt_template: 'USER: {} ASSISTANT: '
  system_message: "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions." 
  merged_ckpt: "./weights/llava-v1.5-7b"
  
  # ðŸ”¥ NEW for 4-bit quantization
  quantization:
    load_in_4bit: True
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_use_double_quant: True
    bnb_4bit_quant_type: "nf4"

  # ðŸ”§ Device/offloading
  device_map: "auto"
  offload_folder: "offload"
  
datasets:
  cc_sbu_align:
    vis_processor:
      train:
        name: "clip_image_eval"
        proc_type: "openai/clip-vit-large-patch14-336"
    text_processor:
      train:
        name: "blip_caption"

run:
  task: image_text_pretrain
  seed: 42
